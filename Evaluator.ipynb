{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Comparison\n",
        "\n",
        "This notebook evaluates multiple trained models and compares them against a baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "from evaluate import evaluate\n",
        "import pandas as pd\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the models you want to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"data/\"\n",
        "ASSETS_PATH = \"assets/\"\n",
        "\n",
        "# List of model names to evaluate (these should exist in assets/ directory)\n",
        "# first one will be the baseline\n",
        "MODEL_NAMES = [\n",
        "    \"logistic_regression_baseline\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EVALUATION SETUP\n",
            "================================================================================\n",
            "✗ GPU not available, using CPU\n",
            "✓ Device: cpu\n",
            "================================================================================\n",
            "Loaded label mapping from assets/label_mapping.csv\n",
            "Created training dataloader with 2416 samples\n",
            "Created validation dataloader with 658 samples\n",
            "Created test dataloader with 345 samples\n",
            "Loading test data...\n",
            "Loaded 345 test samples\n",
            "\n",
            "Evaluating model: logistic_regression_baseline\n",
            "  Top-1 Accuracy: 84.64%\n",
            "  Top-3 Accuracy: 96.81%\n",
            "  Loss: 0.5254\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "results_df = evaluate(DATA_PATH, MODEL_NAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretty Print Results with Baseline Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretty_print_results(results_df, decimal_places=3):\n",
        "    \"\"\"\n",
        "    Pretty print evaluation results with baseline comparison.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with evaluation results\n",
        "        decimal_places: Number of decimal places for rounding\n",
        "    \"\"\"\n",
        "    # Find baseline row\n",
        "    baseline_metrics = results_df.iloc[0]\n",
        "\n",
        "    baseline_name = baseline_metrics['model_name']\n",
        "    \n",
        "    # Get numeric columns (excluding model_name)\n",
        "    numeric_cols = [col for col in results_df.columns if col != 'model_name']\n",
        "    \n",
        "    print(\"=\" * 100)\n",
        "    print(\"EVALUATION RESULTS WITH BASELINE COMPARISON\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"\\nBaseline Model: {baseline_name}\")\n",
        "    print(\"-\" * 100)\n",
        "    \n",
        "    # Display each model's results\n",
        "    for idx, row in results_df.iterrows():\n",
        "        model_name = row['model_name']\n",
        "        is_baseline = model_name == baseline_name\n",
        "        \n",
        "        print(f\"\\n{'> BASELINE <' if is_baseline else 'Model'}: {model_name}\")\n",
        "        print(\"-\" * 100)\n",
        "        \n",
        "        for col in numeric_cols:\n",
        "            value = row[col]\n",
        "            \n",
        "            # Skip loss and accuracy columns for percent difference calculation\n",
        "            if 'loss' in col.lower():\n",
        "                print(f\"  {col:40s}: {value:.{decimal_places}f}\")\n",
        "            elif 'accuracy' in col.lower() or 'precision' in col.lower() or 'recall' in col.lower():\n",
        "                baseline_value = baseline_metrics[col]\n",
        "                \n",
        "                if is_baseline:\n",
        "                    print(f\"  {col:40s}: {value:.{decimal_places}f}%\")\n",
        "                else:\n",
        "                    diff = value - baseline_value\n",
        "                    diff_percent = (diff / baseline_value * 100) if baseline_value != 0 else 0\n",
        "                    sign = \"+\" if diff >= 0 else \"\"\n",
        "                    print(f\"  {col:40s}: {value:.{decimal_places}f}% ({sign}{diff_percent:.{decimal_places}f}% vs baseline)\")\n",
        "            else:\n",
        "                print(f\"  {col:40s}: {value:.{decimal_places}f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "EVALUATION RESULTS WITH BASELINE COMPARISON\n",
            "====================================================================================================\n",
            "\n",
            "Baseline Model: logistic_regression_baseline\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "> BASELINE <: logistic_regression_baseline\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  top1_accuracy                           : 84.638%\n",
            "  top3_accuracy                           : 96.812%\n",
            "  loss                                    : 0.525\n",
            "  precision_asteroid                      : 75.862%\n",
            "  recall_asteroid                         : 75.862%\n",
            "  precision_black_hole                    : 70.588%\n",
            "  recall_black_hole                       : 77.419%\n",
            "  precision_earth                         : 83.333%\n",
            "  recall_earth                            : 86.207%\n",
            "  precision_galaxy                        : 84.848%\n",
            "  recall_galaxy                           : 93.333%\n",
            "  precision_jupiter                       : 65.517%\n",
            "  recall_jupiter                          : 67.857%\n",
            "  precision_mars                          : 90.000%\n",
            "  recall_mars                             : 90.000%\n",
            "  precision_mercury                       : 85.714%\n",
            "  recall_mercury                          : 85.714%\n",
            "  precision_neptune                       : 92.857%\n",
            "  recall_neptune                          : 92.857%\n",
            "  precision_pluto                         : 95.455%\n",
            "  recall_pluto                            : 75.000%\n",
            "  precision_saturn                        : 80.000%\n",
            "  recall_saturn                           : 85.714%\n",
            "  precision_uranus                        : 100.000%\n",
            "  recall_uranus                           : 96.429%\n",
            "  precision_venus                         : 100.000%\n",
            "  recall_venus                            : 89.286%\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display pretty results\n",
        "pretty_print_results(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
