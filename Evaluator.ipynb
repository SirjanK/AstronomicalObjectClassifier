{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Comparison\n",
        "\n",
        "This notebook evaluates multiple trained models and compares them against a baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "from evaluate import evaluate\n",
        "import pandas as pd\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the models you want to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"data/\"\n",
        "ASSETS_PATH = \"assets/\"\n",
        "\n",
        "# List of model names to evaluate (these should exist in assets/ directory)\n",
        "# first one will be the baseline\n",
        "MODEL_NAMES = [\n",
        "    \"logistic_regression_100_epochs\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EVALUATION SETUP\n",
            "================================================================================\n",
            "✗ GPU not available, using CPU\n",
            "✓ Device: cpu\n",
            "================================================================================\n",
            "Loaded label mapping from assets/label_mapping.csv\n",
            "Created training dataloader with 2416 samples\n",
            "Created validation dataloader with 658 samples\n",
            "Created test dataloader with 345 samples\n",
            "Loading test data...\n",
            "Loaded 345 test samples\n",
            "\n",
            "Evaluating model: logistic_regression_100_epochs\n",
            "  Top-1 Accuracy: 86.09%\n",
            "  Top-3 Accuracy: 97.68%\n",
            "  Loss: 0.4353\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "results_df = evaluate(DATA_PATH, MODEL_NAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pretty Print Results with Baseline Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretty_print_results(results_df, decimal_places=3):\n",
        "    \"\"\"\n",
        "    Pretty print evaluation results with models as columns for easy comparison.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with evaluation results\n",
        "        decimal_places: Number of decimal places for rounding\n",
        "    \"\"\"\n",
        "    baseline_model_name = results_df.iloc[0]['model_name']\n",
        "    num_models = len(results_df)\n",
        "    \n",
        "    # Get numeric columns (excluding model_name)\n",
        "    numeric_cols = [col for col in results_df.columns if col != 'model_name']\n",
        "    \n",
        "    # Get list of metric categories\n",
        "    base_metrics = ['top1_accuracy', 'top3_accuracy', 'loss']\n",
        "    class_cols = [col for col in numeric_cols if col not in base_metrics]\n",
        "    \n",
        "    # Build base metrics DataFrame\n",
        "    base_data = []\n",
        "    for metric in base_metrics:\n",
        "        row = {'Metric': metric}\n",
        "        for idx, model_row in results_df.iterrows():\n",
        "            model_name = model_row['model_name']\n",
        "            value = model_row[metric]\n",
        "            baseline_val = results_df[results_df['model_name'] == baseline_model_name].iloc[0][metric]\n",
        "            \n",
        "            if model_name != baseline_model_name:\n",
        "                diff = value - baseline_val\n",
        "                if 'loss' in metric:\n",
        "                    diff_percent = abs(diff / baseline_val * 100) if baseline_val != 0 else 0\n",
        "                    sign = \"↓\" if diff < 0 else \"↑\"\n",
        "                    formatted = f\"{value:.{decimal_places}f} {sign}{diff_percent:.{decimal_places}f}%\"\n",
        "                else:\n",
        "                    diff_percent = (diff / baseline_val * 100) if baseline_val != 0 else 0\n",
        "                    sign = \"↓\" if diff < 0 else \"↑\"\n",
        "                    formatted = f\"{value:.{decimal_places}f}% {sign}{diff_percent:.{decimal_places}f}%\"\n",
        "            else:\n",
        "                if 'loss' in metric:\n",
        "                    formatted = f\"{value:.{decimal_places}f}\"\n",
        "                else:\n",
        "                    formatted = f\"{value:.{decimal_places}f}%\"\n",
        "            row[model_name] = formatted\n",
        "        base_data.append(row)\n",
        "    \n",
        "    base_df = pd.DataFrame(base_data)\n",
        "    base_df = base_df.set_index('Metric')\n",
        "    \n",
        "    # Build per-class metrics DataFrame\n",
        "    class_data = []\n",
        "    processed_combos = set()\n",
        "    \n",
        "    for col in class_cols:\n",
        "        if 'precision' in col:\n",
        "            class_name = col.replace('precision_', '')\n",
        "            metric_type = 'precision'\n",
        "        elif 'recall' in col:\n",
        "            class_name = col.replace('recall_', '')\n",
        "            metric_type = 'recall'\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        combo = (class_name, metric_type)\n",
        "        if combo in processed_combos:\n",
        "            continue\n",
        "        processed_combos.add(combo)\n",
        "        \n",
        "        row = {'Class': class_name, 'Metric': metric_type}\n",
        "        metric_col = f\"{metric_type}_{class_name}\"\n",
        "        \n",
        "        if metric_col not in results_df.columns:\n",
        "            continue\n",
        "        \n",
        "        for idx, model_row in results_df.iterrows():\n",
        "            model_name = model_row['model_name']\n",
        "            value = model_row[metric_col]\n",
        "            baseline_val = results_df[results_df['model_name'] == baseline_model_name].iloc[0][metric_col]\n",
        "            \n",
        "            if model_name != baseline_model_name:\n",
        "                diff = value - baseline_val\n",
        "                diff_percent = (diff / baseline_val * 100) if baseline_val != 0 else 0\n",
        "                sign = \"↓\" if diff < 0 else \"↑\"\n",
        "                formatted = f\"{value:.{decimal_places}f}% {sign}{diff_percent:.{decimal_places}f}%\"\n",
        "            else:\n",
        "                formatted = f\"{value:.{decimal_places}f}%\"\n",
        "            \n",
        "            row[model_name] = formatted\n",
        "        class_data.append(row)\n",
        "    \n",
        "    class_df = pd.DataFrame(class_data)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 120)\n",
        "    print(\"EVALUATION RESULTS - SIDE BY SIDE COMPARISON\")\n",
        "    print(\"=\" * 120)\n",
        "    print(f\"Baseline: {baseline_model_name}\")\n",
        "    print(\"=\" * 120 + \"\\n\")\n",
        "    \n",
        "    print(\"BASE METRICS:\")\n",
        "    print(\"-\" * 120)\n",
        "    print(base_df.to_string())\n",
        "    print()\n",
        "    \n",
        "    if len(class_data) > 0:\n",
        "        print(\"PER-CLASS METRICS:\")\n",
        "        print(\"-\" * 120)\n",
        "        print(class_df.to_string(index=False))\n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================================================================================\n",
            "EVALUATION RESULTS - SIDE BY SIDE COMPARISON\n",
            "========================================================================================================================\n",
            "Baseline: logistic_regression_100_epochs\n",
            "========================================================================================================================\n",
            "\n",
            "BASE METRICS:\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "              logistic_regression_100_epochs\n",
            "Metric                                      \n",
            "top1_accuracy                        86.087%\n",
            "top3_accuracy                        97.681%\n",
            "loss                                   0.435\n",
            "\n",
            "PER-CLASS METRICS:\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "     Class    Metric logistic_regression_100_epochs\n",
            "  asteroid precision                        90.909%\n",
            "  asteroid    recall                        68.966%\n",
            "black_hole precision                        68.421%\n",
            "black_hole    recall                        83.871%\n",
            "     earth precision                        79.412%\n",
            "     earth    recall                        93.103%\n",
            "    galaxy precision                        87.500%\n",
            "    galaxy    recall                        93.333%\n",
            "   jupiter precision                        70.370%\n",
            "   jupiter    recall                        67.857%\n",
            "      mars precision                        93.103%\n",
            "      mars    recall                        90.000%\n",
            "   mercury precision                        85.714%\n",
            "   mercury    recall                        85.714%\n",
            "   neptune precision                        92.857%\n",
            "   neptune    recall                        92.857%\n",
            "     pluto precision                        95.833%\n",
            "     pluto    recall                        82.143%\n",
            "    saturn precision                        80.000%\n",
            "    saturn    recall                        85.714%\n",
            "    uranus precision                       100.000%\n",
            "    uranus    recall                        96.429%\n",
            "     venus precision                       100.000%\n",
            "     venus    recall                        92.857%\n",
            "\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display pretty results\n",
        "pretty_print_results(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
